# Задание 1

Иходя из описания проблем компании и её текущей архитектуры можно выделить следующие важные аспекты, влияющие на выбор стратегий масштабирования и отказоусточивости.

* БД является единой точкой отказа т.к. установлена на одной VM
* Отсутствие изоляции - проблемы одного компонента влияют на остальные сервисы
* Нагрузка создаваемая одним партнёром влияет на всех остальных, съедая все ресурсы
* Проблемы с мониторингом и алертингом, потому что об отказах часто узнают от пользователей
* Географическая удалённость - компании важно чтобы пользователи могли одинаково удобно пользоваться приложением независимо от их местоположения - чтобы данные грузились одинаково быстро
* Персональные данные находятся в той же зоне, где и все остальные, что усложняет управление доступом к ним.

## Стратегия масштабирования

### Сервисы

Большим плюсом является то, что компания уже перевела свои сервисы в облако. Это позволит довольно быстро применить горизонтальное масштабирование для сервисов `core-app`, `client-info`, `ins-product-aggregator` и `ins-comp-settlement`. У разных сервисов может использоваться разный коэффициент масштабирования. Реализовать масштабирование на уровне Kubernetes можно с помощью Horizontal Pod Autoscaler.

### БД

На уровне БД для начала применить вертикальное масштабирование с репликацией с Master и хотя бы одной Standby репликой. При чём желательно сразу создать отдельные базы для каждого сервиса. Это можно сделать относительно безоболезенно так как у каждого сервиса есть своя схема. Реплику можно использовать для аналитических read-only запросов. В дальнейшем рассмотреть реализацию горизонтального шардирования БД (например, по регионам или клиентам).

### Фронтенд

На уровне фронт-енда целесообразно подумать о настройке `CDN` разместив туда статику. А для динамических запросов можно реализовать кэширование на уровне `API Gateway`.

## Стратегия отказоустойчивости

Для начала необходимо обеспечить стратегию Active-Standby. Но если появится экономическая целесообразность, то в будущем серьёзно рассмотреть воозможность перехода на Active-Active. Active-Standby подход с ростом нагрузки не сможет обеспечивать требования доступности в 99.9% потому что при переключении на резервные компоненты возможны задержки - например из-за необходимости прогреть кэш или горизонтально масштабироваться под нагрузку.

Для Standby подготовить ноды в Kubernetes в разных зонах доступности. Например, если главную ноду расположить в Москве, то Standby ноды можно расположить в Екатеринбурге и Новосибирске. Между регионами настроить асинхронную репликацию. Здесь имеются ввиду именно независимые кластеры, а не один растянутый. Независимые кластеры позволят излировать сбои в разных регионах. Кроме того ими проще управлять и гибко применять версионирование для тестирования обновлений в одном регионе, а затем распространять их на остальные. И в дополнение мы получаем чёткое распределение трат по регионам.

Также настроить GeoDNS для направления трафика в ближайший регион.

На уровне API Gateway критично настроить Rate Limiting для контроля RPS от разных партнёров чтобы избавиться от проблем, когда один партнёр захватывает все ресурсы.

Фейловер БД настроить автоматичсекий через Patroni.

Ожидается, что всё расходы на обеспечение отказоустойчивости быстро окупятся, т.к. простой системы обходится в 500.000 рублей в час.

И очень важным аспектом обеспечения отказоустойчивости, а конкретно - способности быстро восстанавливать работоспособность сервисов, должны стать настроенные мониторинг и алертинг. Для этого необходимо внедрить Prometheus + Grafana + любой удобный инструмент алертинга. Например в Grafana настроить автоматический алертинг по событиям в мессенджеры или на почту поддержки.

## Зоны безопасности

Предлагается разделить систему на две зоны безопасности - публичную и конфиденциальную.

В конфиденциальну зону выделить сервис client-info с изолированным деплоем и отдельным PostgreSQL кластером. Внутр кластера можно будет применять шифрование данных, настраивать независимую политику бэкапов и создавать реплики.

Также для доступа к кониденциальной зоне необходимо прописать отдельные настройки в API Gateway - mTLS, использование JWT токенов. Также не помешает сделать отдельны настройки по Rate limiting. В будущем можно рассмотреть целесообразность подключения движка политик аля Open Policy Agent.

В качестве дополнительной защиты настроить мониторинг аномального поведения. Например, частые 403 ошибки, запросы к данным чужих пользователей, запросы полного экспорта данных и т.п.

## Итог

Вышеперечисленные меры позволят гибко масштабировать сервисы и их хранилища, а также отдельно обеспечить безопасность персональных данных клиентов.
